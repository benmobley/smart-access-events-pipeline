# Smart Access Events Pipeline

End-to-end IoT analytics pipeline simulating smart garage door telemetry with **real-time streaming** (Kafka) and **batch processing** (Airflow). Built with dbt transformations and Streamlit visualization.

**Inspired by**: Chamberlain Group's myQ ecosystem

---

## ğŸš€ Quick Start

**Prerequisites**: Python 3.9+ Â· PostgreSQL Â· Docker

### **Streaming Workflow** (Real-Time)

```bash
# 1. Setup environment
git clone https://github.com/benmobley/smart-access-events-pipeline.git
cd smart-access-events-pipeline
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt

# 2. Create database and load initial schema
createdb smart_access
python etl/generate_synthetic_data.py
python etl/load_to_postgres.py

# 3. Start Kafka
docker-compose -f docker-compose.kafka.yml up -d

# 4. Run streaming pipeline (2 terminals)
python streaming/kafka_consumer.py    # Terminal 1: Consumes from Kafka â†’ PostgreSQL
python streaming/kafka_producer.py    # Terminal 2: Generates events â†’ Kafka

# 5. Transform and visualize
cd smart_access_dbt && dbt run && cd ..
streamlit run analytics/streamlit_app.py
```

**Access**: Dashboard http://localhost:8501 | Kafka UI http://localhost:8090

ğŸ“– **[Full Streaming Guide â†’](streaming/README.md)**

---

### **Batch Workflow** (Scheduled)

**Option 1: Airflow** (runs daily at 2 AM UTC)

```bash
docker-compose -f docker-compose.airflow.yml --env-file .env.airflow up -d
# Airflow UI: http://localhost:8080 (airflow/airflow)
```

**Option 2: Manual Bash Script**

```bash
./orchestration/run_all.sh
streamlit run analytics/streamlit_app.py
```

---

## ğŸ“Š Architecture

### **Data Flow**

**Streaming**: IoT Simulator â†’ Kafka Topics â†’ Consumer â†’ PostgreSQL â†’ dbt (incremental) â†’ Dashboard  
**Batch**: Synthetic Generator â†’ CSV Files â†’ PostgreSQL â†’ dbt (full refresh) â†’ Dashboard

### **Data Models** (Star Schema)

**Raw** (`public` schema): Landing tables for events, devices, households, health metrics

**Staging** (`smart_access` schema): Cleaned, standardized views

**Marts** (`smart_access` schema):

- **Dimensions**: `dim_device`, `dim_household`
- **Facts**: `fct_access_events` (event-level), `fct_device_daily_summary` (aggregated)

---

## ğŸ› ï¸ Tech Stack

**Streaming**: Kafka Â· Zookeeper Â· kafka-python  
**Orchestration**: Airflow Â· Docker Compose  
**ETL**: Python Â· Faker Â· pandas Â· SQLAlchemy  
**Database**: PostgreSQL  
**Transformation**: dbt Core  
**Visualization**: Streamlit Â· Plotly

---

## ğŸ“ˆ Analytics Capabilities

**Metrics**: Event volumes, failure rates, device health (battery/signal), online ratios  
**Visualizations**: Time series trends, failure analysis by model/firmware, health monitoring  
**Filtering**: Date ranges, regions, device types/models  
**Use Cases**: Predictive maintenance, operational monitoring, user behavior analysis

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ streaming/           # Kafka producer/consumer, real-time ingestion
â”œâ”€â”€ airflow/dags/        # Batch (daily) and streaming (15min) DAGs
â”œâ”€â”€ etl/                 # Synthetic data generation and CSV loading
â”œâ”€â”€ smart_access_dbt/    # dbt models (staging + marts)
â”œâ”€â”€ analytics/           # Streamlit dashboard
â”œâ”€â”€ orchestration/       # Bash automation script
â””â”€â”€ docker-compose.*     # Kafka and Airflow infrastructure
```

---

## ğŸ§ª Data Quality

18 dbt tests validate uniqueness, not-null constraints, referential integrity, and accepted values.

---

## ğŸ”® Roadmap

- âœ… Kafka real-time streaming
- ğŸ”„ Incremental dbt models for streaming data
- ğŸ”„ SCD Type 2 for dimension history tracking
- ğŸ”„ Predictive maintenance ML models
- ğŸ”„ Data quality monitoring (Great Expectations)

---

**Portfolio Project** | Modern data engineering with streaming and batch architectures
